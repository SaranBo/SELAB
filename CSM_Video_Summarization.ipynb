{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPIcxdAQPwo3ZcEKc/AniGu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaranBo/SELAB/blob/main/CSM_Video_Summarization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "aEmYllTYeq5C"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, GRU\n",
        "from keras.applications import ResNet50\n",
        "import cv2\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We shall use the function extract_frames to take video files as input, extract frames from the video at a specific interval (e.g., one frame per second).\n",
        "and finally resize frames to a uniform dimension suitable for the neural network."
      ],
      "metadata": {
        "id": "sMJE9p-VfMIr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_frames(video_path, interval=1):\n",
        "    \"\"\"\n",
        "    Extracts frames from a video at a specified interval.\n",
        "\n",
        "    Args:\n",
        "    - video_path (str): The path to the video file.\n",
        "    - interval (int): Interval in seconds at which to extract frames.\n",
        "\n",
        "    Returns:\n",
        "    - List of frames (as numpy arrays).\n",
        "    \"\"\"\n",
        "\n",
        "    #input\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video.\")\n",
        "        return []\n",
        "\n",
        "    frames = []\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))  # Fps of video\n",
        "    frame_count = 0\n",
        "\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "\n",
        "        # if no frame break loop\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # extract frame if at interval specified\n",
        "        if frame_count % (fps * interval) == 0:\n",
        "            frames.append(frame)\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    # release video capture object\n",
        "    cap.release()\n",
        "\n",
        "    return frames\n"
      ],
      "metadata": {
        "id": "JSYh_JvDfCy9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we shall define a function called preprocess_frames which shall\n",
        "\n",
        "Iterates through each frame in the given list of frames.\n",
        "\n",
        "Resizes each frame to the specified target_size. Here, cv2.INTER_AREA interpolation is used, which is effective for image shrinking.\n",
        "\n",
        "Normalizes the pixel values of each frame to the range [0, 1] by dividing by 255.0. This normalization is a common practice when working with neural\n",
        "networks, as it helps to stabilize the training process.\n",
        "\n",
        "Appends the processed frame to a new list, which is then converted to a numpy array before being returned.\n"
      ],
      "metadata": {
        "id": "IbZhn0d8f8Lo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_frames(frames, target_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Preprocesses video frames by resizing and normalizing them.\n",
        "\n",
        "    Args:\n",
        "    - frames (list): A list of frames (as numpy arrays).\n",
        "    - target_size (tuple): The target size for resizing frames, default is (224, 224).\n",
        "\n",
        "    Returns:\n",
        "    - Preprocessed frames as a numpy array.\n",
        "    \"\"\"\n",
        "\n",
        "    preprocessed_frames = []\n",
        "\n",
        "    for frame in frames:\n",
        "        # Resize frame\n",
        "        resized_frame = cv2.resize(frame, target_size, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "        # Normalize pixel values to be between 0 and 1\n",
        "        normalized_frame = resized_frame / 255.0\n",
        "\n",
        "        preprocessed_frames.append(normalized_frame)\n",
        "\n",
        "    # Convert list of frames to a numpy array\n",
        "    preprocessed_frames = np.array(preprocessed_frames)\n",
        "\n",
        "    return preprocessed_frames\n"
      ],
      "metadata": {
        "id": "5t-lFB9sfK-t"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The build_rcnn_feature_extractor function constructs a Recurrent Convolutional Neural Network (RCNN) for video processing by combining a pre-trained Convolutional Neural Network (CNN) with Recurrent Neural Network (RNN) layers. It starts with ResNet50, a widely-used CNN model pre-trained on ImageNet, for extracting spatial features from video frames. These features are then fed into RNN layers, encapsulated within a TimeDistributed wrapper, allowing the model to process a sequence of frames and capture temporal dynamics. This hybrid approach of spatial and temporal feature extraction makes it ideal for tasks like video classification or summarization, and it can be seamlessly integrated and executed in a Google Colab environment, leveraging TensorFlow and Keras libraries"
      ],
      "metadata": {
        "id": "Q3FHgTk_iK1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cvKMGr1bkNA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_feature_extractor():\n",
        "    \"\"\"\n",
        "    Builds a feature extractor using a pre-trained CNN (ResNet50).\n",
        "\n",
        "    Returns:\n",
        "    - A Keras model that takes an image as input and outputs feature vectors.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load the pre-trained ResNet50 model\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False)\n",
        "\n",
        "    # Create a custom model that outputs features from the pre-trained model\n",
        "    # We use the output of the last convolutional layer\n",
        "    output = base_model.layers[-1].output\n",
        "\n",
        "    # Construct the new model\n",
        "    feature_extractor = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    return feature_extractor\n"
      ],
      "metadata": {
        "id": "TN2TlOYSkNas"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "def build_sequence_model(input_shape):\n",
        "    \"\"\"\n",
        "    Builds a sequence model using RNN (LSTM).\n",
        "\n",
        "    Args:\n",
        "    - input_shape (tuple): The shape of the input features (output of CNN).\n",
        "\n",
        "    Returns:\n",
        "    - A Keras model designed for sequence processing.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Adding an LSTM layer\n",
        "    # You can tweak the number of units and other parameters as needed\n",
        "    model.add(LSTM(256, input_shape=input_shape, return_sequences=False))\n",
        "\n",
        "    # Adding a Dense layer for some additional processing\n",
        "    # You can modify the number of units or add more layers as required\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    # The output dimension depends on the subsequent use-case (e.g., text generation)\n",
        "    model.add(Dense(64, activation='relu'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "vjZCtqfjkOfu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "\n",
        "def build_text_generator(input_shape):\n",
        "    \"\"\"\n",
        "    Builds a text generator model.\n",
        "\n",
        "    Args:\n",
        "    - input_shape (tuple): The shape of the input vector (output of sequence model).\n",
        "\n",
        "    Returns:\n",
        "    - A Keras model designed for text generation.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    # Input layer\n",
        "    model.add(Dense(256, activation='relu', input_shape=input_shape))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Hidden layers\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "\n",
        "    # Output layer\n",
        "    # The number of units in the output layer should correspond to the size of your text vocabulary\n",
        "    # For simplicity, let's assume a hypothetical vocabulary size of 1000\n",
        "    model.add(Dense(1000, activation='softmax'))\n",
        "\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "eqRU-R6ulN6q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(video_path):\n",
        "    \"\"\"\n",
        "    Generates a textual summary for a given video using advanced methods with integrated sequence modeling.\n",
        "\n",
        "    Args:\n",
        "    - video_path (str): The path to the video file.\n",
        "\n",
        "    Returns:\n",
        "    - The generated textual summary as a string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Extract and preprocess frames from the video\n",
        "    frames = extract_frames(video_path)\n",
        "    preprocessed_frames = preprocess_frames(frames)\n",
        "\n",
        "    # Step 2: Feature Extraction\n",
        "    feature_extractor = build_feature_extractor()\n",
        "    features = feature_extractor.predict(preprocessed_frames)\n",
        "\n",
        "    # Flatten the features to simplify the example\n",
        "    flattened_features = features.reshape((features.shape[0], -1))\n",
        "\n",
        "    # Step 3: Sequence Modeling\n",
        "    # Utilizing the build_sequence_model function for sequence modeling\n",
        "    sequence_model = build_sequence_model(flattened_features.shape[1:])\n",
        "    video_representation = sequence_model.predict(flattened_features[np.newaxis, ...])\n",
        "\n",
        "    # Step 4: Text Generation\n",
        "    # Use a pre-trained language model for text generation\n",
        "    # For demonstration, let's use GPT-2 model via Hugging Face's transformers library\n",
        "    summarizer = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "    summary = summarizer(video_representation.flatten().tolist(), max_length=50, min_length=25, do_sample=False)[0]['generated_text']\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "RRt1kX6VlQEJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBvkx6q8rHTc",
        "outputId": "28ecbd34-252f-4938-9b40-ccbe97e4e2b2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "R2VH8XjCvwiP",
        "outputId": "63159348-10b0-4661-d85f-0e0bdcf85f3f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c36e1289-d810-4572-85c1-c14a9a318d05\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c36e1289-d810-4572-85c1-c14a9a318d05\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Wildlife Windows 7 Sample Video.mp4 to Wildlife Windows 7 Sample Video.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "video_path = '/content/Wildlife Windows 7 Sample Video.mp4'  # Path to the uploaded video file\n",
        "summary = generate_summary(video_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "uax9HuCex6OB",
        "outputId": "f1f8612f-b2b2-4abc-8d37-49a150194714"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 9s 9s/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6823100f7b36>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/Wildlife Windows 7 Sample Video.mp4'\u001b[0m  \u001b[0;31m# Path to the uploaded video file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-aee9172755db>\u001b[0m in \u001b[0;36mgenerate_summary\u001b[0;34m(video_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Step 3: Sequence Modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Utilizing the build_sequence_model function for sequence modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msequence_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_sequence_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mvideo_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflattened_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-9debbccb4fac>\u001b[0m in \u001b[0;36mbuild_sequence_model\u001b[0;34m(input_shape)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Adding an LSTM layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# You can tweak the number of units and other parameters as needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Adding a Dense layer for some additional processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/trackable/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    236\u001b[0m                     \u001b[0;34mf'Input {input_index} of layer \"{layer_name}\" '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m                     \u001b[0;34m\"is incompatible with the layer: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input 0 of layer \"lstm\" is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 100352)"
          ]
        }
      ]
    }
  ]
}